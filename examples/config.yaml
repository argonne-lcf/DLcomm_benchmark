framework        : pytorch
ccl_backend      : ccl
extended_logging : off
barrier          : on

order_of_run: [fsdp, sp]

fsdp: # Task Name
  comm_group: within_node
  num_compute_nodes: 2
  num_gpus_per_node: 4
  gpu_ids_per_node: [0,1,2,3]
  verify_correctness: on
  collective:
    collective_name: allreduce
    collective_op: sum
    scale_up_algorithm: default
    scale_out_algorithm: default
    iterations: 10
    warmup_iterations: 0
    add_mxm_compute: on
    payload:
      dtype: float32
      count: 
      buffer_size: 1GB

sp: # Task Name 
  comm_group: within_node
  num_compute_nodes: 2
  num_gpus_per_node: 4
  gpu_ids_per_node: [0,1,2,3]
  verify_correctness: on
  collective:
    collective_name: allgather
    collective_op: sum
    scale_up_algorithm: default
    scale_out_algorithm: default
    iterations: 10
    warmup_iterations: 0
    add_mxm_compute: on
    payload:
      dtype: float32
      count: 
      buffer_size: 1GB
    