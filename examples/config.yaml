framework  : pytorch  # tensorflow / jax / monarch
ccl_backend : ccl   # rccl / nccl / xccl (Note: PyTorch 2.7+ users should use 'xccl' instead of 'ccl' for Intel oneCCL)
ccl_debug   : on # on / off - enables CCL debug logging and algorithm selection reporting
use_profiler: unitrace
barrier     : on   # on / off - on: adds MPI barrier before timer printing for accurate timing, off: only rank 0 prints (other collectives may still be in process)

  
order_of_run: [ULYSS-alltoallsingle, SP-FSDP-allgather, SP-FSDP-reducescatter, grad-sync, flat]  # Can be single implementation or list of implementations
# Examples:
# order_of_run: ULYSS                      # Single implementation
# order_of_run: [ULYSS, SP]                # Multi-implementation execution
# order_of_run: [ULYSS, SP, Flat]          # All implementations

implementations:
  - name: ULYSS-alltoallsingle
    comm_groups:

      within_node: 
        num_compute_nodes: 2 
        num_gpus_per_node: 4
        gpu_ids_per_node:  [0,1,2,3]
        collective:
          name: reduce  # allgather / reducescatter / broadcast / alltoall / alltoallsingle
          op:           # max / min / prod / sum
          scale_up_algorithm: direct
          scale_out_algorithm:        # rabinseifner 
          iterations: 5
          warmup_iterations: 0
          payload:
            dtype: bfloat16  # float64 / int32 / int64 / bfloat16 / float8 / float32
            count: 
            buffer_size: 1KB # 4096  # in Bytes -> float32(4B) x 1024 elements
        verify_correctness: on

  - name: SP-FSDP-allgather
    comm_groups:

      within_node: 
        num_compute_nodes: 2 
        num_gpus_per_node: 3
        gpu_ids_per_node:  [0,1,3]    
        collective:
          name: allgather   # allgather / reducescatter / broadcast / alltoall / alltoallsingle
          op:           # max / min / prod / sum
          scale_up_algorithm: ring
          scale_out_algorithm: ring        # rabinseifner 
          iterations: 5
          warmup_iterations: 0
          payload:
            dtype: bfloat16  # float64 / int32 / int64 / bfloat16 / float8 / float32
            count: 1024
            buffer_size: # 4096  # in Bytes -> float32(4B) x 1024 elements
        verify_correctness: on

  - name: SP-FSDP-reducescatter
    comm_groups:

      within_node: 
        num_compute_nodes: 2 
        num_gpus_per_node: 4
        gpu_ids_per_node:  [0,1,2,3]      
        collective:
          name: reducescatter  # allgather / reducescatter / broadcast / alltoall / alltoallsingle
          op:       min     # max / min / prod / sum
          scale_up_algorithm: direct
          scale_out_algorithm: ring        # rabinseifner 
          iterations: 5
          warmup_iterations: 0
          payload:
            dtype: bfloat16  # float64 / int32 / int64 / bfloat16 / float8 / float32
            count:
            buffer_size: 1KB # 4096  # in Bytes -> float32(4B) x 1024 elements
        verify_correctness: on

  - name: grad-sync
    comm_groups:

      across_node: 
        num_compute_nodes: 2 
        num_gpus_per_node: 3
        gpu_ids_per_node:  [0, 1,3]     
        collective:
          name: allreduce   # allgather / reducescatter / broadcast / alltoall / alltoallsingle
          op:     min      # max / min / prod / sum
          scale_up_algorithm: topo
          scale_out_algorithm: topo        # rabinseifner 
          iterations: 5
          warmup_iterations: 0
          payload:
            dtype: bfloat16  # float64 / int32 / int64 / bfloat16 / float8 / float32
            count: 1024
            buffer_size: # 4096  # in Bytes -> float32(4B) x 1024 elements
        verify_correctness: on


  - name: flat
    comm_groups:


      flatview: 
        num_compute_nodes: 2 
        num_gpus_per_node: 3
        gpu_ids_per_node:  [0,1 ,3]   
        collective:
          name: reduce   # allgather / reducescatter / broadcast / alltoall / alltoallsingle
          op:     min      # max / min / prod / sum
          scale_up_algorithm: direct
          scale_out_algorithm: ring       # rabinseifner 
          iterations: 5
          warmup_iterations: 0
          payload:
            dtype: bfloat16  # float64 / int32 / int64 / bfloat16 / float8 / float32
            count: 
            buffer_size: 1KB # 4096  # in Bytes -> float32(4B) x 1024 elements
        verify_correctness: on


