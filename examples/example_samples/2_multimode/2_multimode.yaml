framework  : pytorch
ccl_backend : nccl
ccl_debug   : on
use_profiler: unitrace
barrier     : on

order_of_run: [within-node-allgather, across-node-allreduce]

implementations:
  - name: within-node-allgather
    comm_groups:
      within_node: 
        num_compute_nodes: 1 
        num_gpus_per_node: 4
        gpu_ids_per_node:  [0,1,2,3]   
        collective:
          name: allgather
          op: 
          scale_up_algorithm: default
          scale_out_algorithm: default
          iterations: 5
          warmup_iterations: 2
          payload:
            dtype: float32
            count: 
            buffer_size: 512KB
        verify_correctness: on

  - name: across-node-allreduce
    comm_groups:
      across_node: 
        num_compute_nodes: 1
        num_gpus_per_node: 4
        gpu_ids_per_node:  [0,1,2,3]   
        collective:
          name: allreduce
          op: sum
          scale_up_algorithm: default
          scale_out_algorithm: default
          iterations: 5
          warmup_iterations: 2
          payload:
            dtype: float32
            count: 
            buffer_size: 512KB
        verify_correctness: on