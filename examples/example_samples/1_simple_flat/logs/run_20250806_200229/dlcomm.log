2025-08-06 20:02:32.339 - DL_COMM - INFO - -------------------------------------------------------------------------
2025-08-06 20:02:32.340 - DL_COMM - INFO - [CONFIG] Loading schema and validating user YAML
2025-08-06 20:02:32.341 - DL_COMM - INFO - [DEBUG] Current working directory: /lus/eagle/projects/datascience_collab/mcim/workspace/DLcomm_benchmark/examples
2025-08-06 20:02:32.341 - DL_COMM - INFO - [DEBUG] Script location: /lus/eagle/projects/datascience_collab/mcim/workspace/DLcomm_benchmark/dl_comm
2025-08-06 20:02:32.341 - DL_COMM - INFO - [DEBUG] Implementations to run: ['simple-allreduce']
2025-08-06 20:02:32.341 - DL_COMM - INFO - [DEBUG] Config loaded successfully
2025-08-06 20:02:32.341 - DL_COMM - INFO - 
2025-08-06 20:02:32.341 - DL_COMM - INFO - [SYSTEM] Collecting environment information...
2025-08-06 20:02:32.341 - DL_COMM - INFO - [SYSTEM] ======================================================
2025-08-06 20:02:32.341 - DL_COMM - INFO - [SYSTEM] PyTorch version: 2.3.0
2025-08-06 20:02:33.757 - DL_COMM - INFO - [SYSTEM] CUDA used to build PyTorch: 12.4
2025-08-06 20:02:33.757 - DL_COMM - INFO - 
2025-08-06 20:02:33.763 - DL_COMM - INFO - [SYSTEM] OS: Linux-5.14.21-150500.55.49-default-x86_64-with-glibc2.31
2025-08-06 20:02:33.768 - DL_COMM - INFO - [SYSTEM] GCC version: gcc (SUSE Linux) 7.5.0
2025-08-06 20:02:33.768 - DL_COMM - INFO - 
2025-08-06 20:02:33.768 - DL_COMM - INFO - [SYSTEM] Python version: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:53:32) [GCC 12.3.0]
2025-08-06 20:02:33.769 - DL_COMM - INFO - [SYSTEM] Python platform: Linux-5.14.21-150500.55.49-default-x86_64-with-glibc2.31
2025-08-06 20:02:33.769 - DL_COMM - INFO - [SYSTEM] Is CUDA available: True
2025-08-06 20:02:33.769 - DL_COMM - INFO - [SYSTEM] CUDA runtime version: 12.4
2025-08-06 20:02:33.769 - DL_COMM - INFO - [SYSTEM] GPU models and configuration:
2025-08-06 20:02:33.834 - DL_COMM - INFO - [SYSTEM] GPU 0: NVIDIA A100-SXM4-40GB (39.4 GB)
2025-08-06 20:02:33.834 - DL_COMM - INFO - [SYSTEM] GPU 1: NVIDIA A100-SXM4-40GB (39.4 GB)
2025-08-06 20:02:33.834 - DL_COMM - INFO - [SYSTEM] GPU 2: NVIDIA A100-SXM4-40GB (39.4 GB)
2025-08-06 20:02:33.834 - DL_COMM - INFO - [SYSTEM] GPU 3: NVIDIA A100-SXM4-40GB (39.4 GB)
2025-08-06 20:02:33.834 - DL_COMM - INFO - 
2025-08-06 20:02:33.834 - DL_COMM - INFO - [SYSTEM] Distributed Backend Availability:
2025-08-06 20:02:33.834 - DL_COMM - INFO - [SYSTEM] NCCL backend available: True
2025-08-06 20:02:33.834 - DL_COMM - INFO - [SYSTEM] MPI backend available: True
2025-08-06 20:02:33.834 - DL_COMM - INFO - [SYSTEM] XCCL backend available: Unknown (API not available)
2025-08-06 20:02:33.834 - DL_COMM - INFO - 
2025-08-06 20:02:33.834 - DL_COMM - INFO - [SYSTEM] Versions of relevant libraries:
2025-08-06 20:02:33.835 - DL_COMM - INFO - [SYSTEM] numpy: 1.26.4
2025-08-06 20:02:33.835 - DL_COMM - INFO - [SYSTEM] mpi4py: 3.1.6
2025-08-06 20:02:33.835 - DL_COMM - INFO - [SYSTEM] hydra-core: 1.3.2
2025-08-06 20:02:33.835 - DL_COMM - INFO - [SYSTEM] omegaconf: 2.3.0
2025-08-06 20:02:33.835 - DL_COMM - INFO - [SYSTEM] oneccl_bindings_for_pytorch: Not available
2025-08-06 20:02:33.836 - DL_COMM - INFO - [SYSTEM] intel_extension_for_pytorch: Not available
2025-08-06 20:02:33.836 - DL_COMM - INFO - [SYSTEM] NCCL version: 2.20.5
2025-08-06 20:02:33.836 - DL_COMM - INFO - 
2025-08-06 20:02:33.836 - DL_COMM - INFO - [SYSTEM] Relevant Environment Variables:
2025-08-06 20:02:33.836 - DL_COMM - INFO - [SYSTEM] CCL_ATL_SHM                    = 1
2025-08-06 20:02:33.836 - DL_COMM - INFO - [SYSTEM] CCL_ATL_TRANSPORT              = ofi
2025-08-06 20:02:33.836 - DL_COMM - INFO - [SYSTEM] CCL_ENABLE_AUTO_CACHE          = 1
2025-08-06 20:02:33.836 - DL_COMM - INFO - [SYSTEM] CCL_ENABLE_PROFILING           = 1
2025-08-06 20:02:33.836 - DL_COMM - INFO - [SYSTEM] CCL_KVS_CONNECTION_TIMEOUT     = 600
2025-08-06 20:02:33.836 - DL_COMM - INFO - [SYSTEM] CCL_KVS_MODE                   = mpi
2025-08-06 20:02:33.836 - DL_COMM - INFO - [SYSTEM] CCL_LOG_LEVEL                  = info
2025-08-06 20:02:33.837 - DL_COMM - INFO - [SYSTEM] CCL_OP_SYNC                    = 1
2025-08-06 20:02:33.837 - DL_COMM - INFO - [SYSTEM] CCL_PROCESS_LAUNCHER           = pmix
2025-08-06 20:02:33.837 - DL_COMM - INFO - [SYSTEM] CUDA_HOME                      = /soft/compilers/cudatoolkit/cuda-12.4.1/
2025-08-06 20:02:33.837 - DL_COMM - INFO - [SYSTEM] CUDA_MODULE_LOADING            = LAZY
2025-08-06 20:02:33.837 - DL_COMM - INFO - [SYSTEM] CUDA_PATH                      = /soft/compilers/cudatoolkit/cuda-12.4.1/
2025-08-06 20:02:33.837 - DL_COMM - INFO - [SYSTEM] CUDA_TOOLKIT_BASE              = /soft/compilers/cudatoolkit/cuda-12.4.1/
2025-08-06 20:02:33.837 - DL_COMM - INFO - [SYSTEM] MPI4JAX_USE_CUDA_MPI           = 1
2025-08-06 20:02:33.837 - DL_COMM - INFO - [SYSTEM] TORCH_CUDA_ARCH_LIST           = 8.0
2025-08-06 20:02:33.837 - DL_COMM - INFO - [SYSTEM] ======================================================
2025-08-06 20:02:33.837 - DL_COMM - INFO - 
2025-08-06 20:02:33.855 - DL_COMM - INFO - 
2025-08-06 20:02:33.856 - DL_COMM - INFO - [MODE 1/1] ---------- FLATVIEW ----------
2025-08-06 20:02:33.856 - DL_COMM - INFO - 
2025-08-06 20:02:33.856 - DL_COMM - INFO - 
2025-08-06 20:02:33.856 - DL_COMM - INFO - [CONFIG] Setup
2025-08-06 20:02:33.856 - DL_COMM - INFO - [CONFIG] ------------------------------------------------------
2025-08-06 20:02:33.856 - DL_COMM - INFO - [CONFIG] Implementation       : simple-allreduce
2025-08-06 20:02:33.857 - DL_COMM - INFO - [CONFIG] Framework            : pytorch
2025-08-06 20:02:33.857 - DL_COMM - INFO - [CONFIG] Backend              : nccl
2025-08-06 20:02:33.857 - DL_COMM - INFO - [CONFIG] Use Profiler         : unitrace
2025-08-06 20:02:33.857 - DL_COMM - INFO - [CONFIG] Barrier Enabled      : True
2025-08-06 20:02:33.857 - DL_COMM - INFO - [CONFIG] World Size           : 4
2025-08-06 20:02:33.857 - DL_COMM - INFO - [CONFIG] ------------------------------------------------------
2025-08-06 20:02:33.857 - DL_COMM - INFO - 
2025-08-06 20:02:33.857 - DL_COMM - INFO - [CONFIG] Communication Group
2025-08-06 20:02:33.857 - DL_COMM - INFO - [CONFIG] ------------------------------------------------------
2025-08-06 20:02:33.857 - DL_COMM - INFO - [CONFIG] Mode                 : flatview
2025-08-06 20:02:33.857 - DL_COMM - INFO - [CONFIG] Topology             : 1 nodes x 4 GPUs
2025-08-06 20:02:33.857 - DL_COMM - INFO - [CONFIG] ------------------------------------------------------
2025-08-06 20:02:33.857 - DL_COMM - INFO - 
2025-08-06 20:02:33.858 - DL_COMM - INFO - [CONFIG] Communication Group Details
2025-08-06 20:02:33.858 - DL_COMM - INFO - [CONFIG] ------------------------------------------------------
2025-08-06 20:02:33.858 - DL_COMM - INFO - [CONFIG] Collective Name      : allreduce
2025-08-06 20:02:33.858 - DL_COMM - INFO - [CONFIG] Operation            : sum
2025-08-06 20:02:33.858 - DL_COMM - INFO - [CONFIG] Scale Up Algorithm   : default
2025-08-06 20:02:33.858 - DL_COMM - INFO - [CONFIG] Scale Out Algorithm  : default
2025-08-06 20:02:33.858 - DL_COMM - INFO - [CONFIG] Data Type            : float32
2025-08-06 20:02:33.858 - DL_COMM - INFO - [CONFIG] Element Count        : 262144
2025-08-06 20:02:33.858 - DL_COMM - INFO - [CONFIG] Buffer Size          : 1MB (1048576 bytes)
2025-08-06 20:02:33.858 - DL_COMM - INFO - [CONFIG] Iterations           : 5
2025-08-06 20:02:33.858 - DL_COMM - INFO - [CONFIG] Verify Correctness   : True
2025-08-06 20:02:33.858 - DL_COMM - INFO - [CONFIG] ------------------------------------------------------
2025-08-06 20:02:33.859 - DL_COMM - INFO - 
2025-08-06 20:02:33.859 - DL_COMM - INFO - [COMM][CONFIG] Flatview: 1 nodes, 4 GPUs per node, Device IDs: [0, 1, 2, 3]
2025-08-06 20:02:33.859 - DL_COMM - INFO - 
2025-08-06 20:02:33.859 - DL_COMM - INFO - [COMM][GROUP CREATION] Flatview groups:
2025-08-06 20:02:33.859 - DL_COMM - INFO - [COMM][GROUP CREATION][Flatview] Ranks: [0, 1, 2, 3], Required GPUs: [0, 1, 2, 3], Logging: rank 0
2025-08-06 20:02:34.770 - DL_COMM - OUTPUT - 
2025-08-06 20:02:34.770 - DL_COMM - OUTPUT - [TIMERS - SETUP] -------------------------------------------
2025-08-06 20:02:34.770 - DL_COMM - OUTPUT - [TIMERS][LOGGING RANK - 0] import time              = 0.000005 s
2025-08-06 20:02:34.770 - DL_COMM - OUTPUT - [TIMERS][LOGGING RANK - 0] init time                = 0.016475 s
2025-08-06 20:02:34.771 - DL_COMM - OUTPUT - [TIMERS - SETUP] -------------------------------------------
2025-08-06 20:02:34.771 - DL_COMM - OUTPUT - 
2025-08-06 20:02:34.771 - DL_COMM - OUTPUT - [MPI] Launching profiling job
2025-08-06 20:02:34.771 - DL_COMM - INFO - 
2025-08-06 20:02:34.771 - DL_COMM - INFO -   [WARMUP] Running 2 warmup iterations...
2025-08-06 20:02:35.019 - DL_COMM - INFO -   [WARMUP] Warmup completed, starting timed iterations...
2025-08-06 20:02:35.020 - DL_COMM - INFO - 
2025-08-06 20:02:36.789 - DL_COMM - OUTPUT - [CORRECTNESS][Flatview-Group-All] AllReduce verification [PASSED] - All 4 ranks received correct values
2025-08-06 20:02:36.799 - DL_COMM - OUTPUT - 
2025-08-06 20:02:36.799 - DL_COMM - OUTPUT - [TIMERS] -------------------------------------------
2025-08-06 20:02:36.799 - DL_COMM - OUTPUT - 
2025-08-06 20:02:36.799 - DL_COMM - INFO -   [TIMERS] [BARRIER ENABLED] Timing measurements used MPI barriers for synchronization
2025-08-06 20:02:36.799 - DL_COMM - OUTPUT - 
2025-08-06 20:02:36.799 - DL_COMM - OUTPUT - [TIMERS] ITERATION TABLE FOR ALLREDUCE:
2025-08-06 20:02:36.799 - DL_COMM - OUTPUT - Iteration        (Flatview)     
2025-08-06 20:02:36.799 - DL_COMM - OUTPUT -               LOGGING RANK - 0  
2025-08-06 20:02:36.800 - DL_COMM - OUTPUT - --------------------------------
2025-08-06 20:02:36.800 - DL_COMM - OUTPUT - 0                 0.001548      
2025-08-06 20:02:36.800 - DL_COMM - OUTPUT - 1                 0.000169      
2025-08-06 20:02:36.800 - DL_COMM - OUTPUT - 2                 0.000117      
2025-08-06 20:02:36.800 - DL_COMM - OUTPUT - 3                 0.000127      
2025-08-06 20:02:36.800 - DL_COMM - OUTPUT - 4                 0.000164      
2025-08-06 20:02:36.800 - DL_COMM - OUTPUT - --------------------------------
2025-08-06 20:02:36.800 - DL_COMM - OUTPUT - 
2025-08-06 20:02:36.800 - DL_COMM - OUTPUT - [TIMERS] -------------------------------------------
2025-08-06 20:02:36.800 - DL_COMM - OUTPUT - 
2025-08-06 20:02:36.800 - DL_COMM - OUTPUT - [BANDWIDTH] -------------------------------------------
2025-08-06 20:02:36.800 - DL_COMM - OUTPUT - [BANDWIDTH] Communication Group Bandwidths:
2025-08-06 20:02:36.801 - DL_COMM - OUTPUT - 
2025-08-06 20:02:36.801 - DL_COMM - OUTPUT - [BANDWIDTH] Flatview:
2025-08-06 20:02:36.801 - DL_COMM - OUTPUT - [BANDWIDTH]   Group Size     : 4 GPUs
2025-08-06 20:02:36.801 - DL_COMM - OUTPUT - [BANDWIDTH]   Buffer Size    : 1048576 bytes
2025-08-06 20:02:36.801 - DL_COMM - OUTPUT - [BANDWIDTH]   Time (iter 0)  : 0.001548 s
2025-08-06 20:02:36.801 - DL_COMM - OUTPUT - [BANDWIDTH]   Bandwidth      : 2709691635 bytes/s
2025-08-06 20:02:36.801 - DL_COMM - OUTPUT - [BANDWIDTH]   Logging Rank   : 0
2025-08-06 20:02:36.801 - DL_COMM - OUTPUT - 
2025-08-06 20:02:36.801 - DL_COMM - OUTPUT - [BANDWIDTH] -------------------------------------------
2025-08-06 20:02:36.801 - DL_COMM - OUTPUT - 
2025-08-06 20:02:36.801 - DL_COMM - INFO - -------------------------------------------------------------------------
2025-08-06 20:02:36.801 - DL_COMM - INFO - [MPI] Job complete
2025-08-06 20:02:36.801 - DL_COMM - INFO - -------------------------------------------------------------------------
2025-08-06 20:02:36.801 - DL_COMM - INFO - Querying Default Table selection
2025-08-06 20:02:36.851 - DL_COMM - INFO - [SELECTION] NCCL algorithm selections for allreduce:
2025-08-06 20:02:36.851 - DL_COMM - INFO - [SELECTION] Algorithm: Ring (protocol: LL128) (user's selection: default)
2025-08-06 20:02:36.851 - DL_COMM - INFO - -------------------------------------------------------------------------
2025-08-06 20:02:36.851 - DL_COMM - INFO - [EXIT] All Done.
2025-08-06 20:02:36.851 - DL_COMM - INFO - -------------------------------------------------------------------------
