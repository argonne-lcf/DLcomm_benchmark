 

torch.ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor
torch.rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) → Tensor
torch.empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False, memory_format=torch.contiguous_format) → Tensor
torch.zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor
torch.full_like(input, fill_value, \*, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor
torch.tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) → Tensor

torch.allclose(input: Tensor, other: Tensor, rtol: float = 1e-05, atol: float = 1e-08, equal_nan: bool = False) → bool

torch.to(device=None, dtype=None, non_blocking=False, copy=False, memory_format=torch.preserve_format) → Tensor

Tensor.clone(*, memory_format=torch.preserve_format) → Tensor

Tensor.sum(dim=None, keepdim=False, dtype=None) → Tensor
 

 
torch.distributed.init_process_group(backend=None, init_method=None, timeout=None, world_size=-1, rank=-1, store=None, group_name='', pg_options=None, device_id=None)
torch.distributed.destroy_process_group(group=None)
torch.distributed.new_group(ranks=None, timeout=None, backend=None, pg_options=None, use_local_synchronization=False, group_desc=None, device_id=None)[source]
torch.distributed.get_process_group_ranks(group)

torch.distributed.get_rank(group=None)
torch.distributed.get_world_size(group=None)

torch.distributed.is_nccl_available()
torch.distributed.is_mpi_available()
torch.distributed.distributed_c10d.is_xccl_available()
 

torch.xpu.is_available()
torch.xpu.device_count()

torch.cuda.is_available()
torch.cuda.device_count()
torch.cuda.get_device_name(device=None)
torch.cuda.get_device_properties(device)

 