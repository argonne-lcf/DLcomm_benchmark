framework: pytorch # tensorflow / jax

collective:
  name: allreduce # allgather / reducescatter / broadcast
  op: sum # max / min 
  algo: ring # tree 

buffer_size: 1MB # 10MB / 100MB

dtype: float32 # float64 / int32 / int64

ccl_backend: xccl # rccl / nccl

horizontal: # Tensor Parallelism
  num_gpus: 12
  gpu_ids: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]

vertical: # Data Parallelism
  num_nodes: 12
  gpu_ids: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
  
use_unitrace: true






