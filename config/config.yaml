framework: pytorch # tensorflow / jax

ccl_backend: xccl # rccl / nccl

collective:
  name: allreduce # allgather / reducescatter / broadcast
  op: sum # max / min 
  algo: ring # tree 
  iterations: 5
  payload:
    buffer_size: 1MB # 10MB / 100MB
    dtype: float32 # float64 / int32 / int64


horizontal: # Tensor Parallelism
  tp_degree: 12
  gpu_ids: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]   

vertical: # Data Parallelism
  dp_degree: 2
  gpu_ids: [0] 
  
flatview: false


use_unitrace: true






